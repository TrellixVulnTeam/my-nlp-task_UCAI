{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Foruck/sentiment-analysis-demo/blob/master/evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bcFtzfTwzVPg",
        "colab_type": "code",
        "outputId": "0a3c90a3-8c82-4c50-ddb4-ec96d28b4beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tEuXeohs0Dyp",
        "colab_type": "code",
        "outputId": "62703651-8cd3-40e9-9985-26bd70d4c761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install psmisc\n",
        "!cp /content/gdrive/My\\ Drive/en_5e-05_2018-12-23_04-48-53.pth en.pth\n",
        "!cp /content/gdrive/My\\ Drive/cn_myBert.pth cn.pth\n",
        "#!mv myBert_cn.pth /content/gdrive/My\\ Drive/cn_myBert.pth \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "psmisc is already the newest version (23.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "cn2input.xml  en2input.xml  gdrive\t      sample_data\n",
            "cn.pth\t      en.pth\t    oup_en2input.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ndngGrU60NjI",
        "colab_type": "code",
        "outputId": "92f2dec2-a66a-4fb5-f661-fd2de8ec79ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install torch torchvision\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "\n",
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.0)\n",
            "Torch 1.0.0 CUDA 9.0.176\n",
            "Device: cuda:0\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.6)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.67)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.1.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.67 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.67)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PS2jrQG90ndz",
        "colab_type": "code",
        "outputId": "573d6829-ae5e-452c-9c67-571c1a0bbc75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import h5py\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertAdam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QAKSlOhH0rex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_Hidden_Size = 256\n",
        "embedding_length = 768\n",
        "Sentence_Max_Length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P6tKhVCc0u0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertTestSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, mask):\n",
        "        self.data = data\n",
        "        self.mask = mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx, :], self.mask[idx, :]\n",
        "\n",
        "class myBert(torch.nn.Module):\n",
        "  def __init__(self, embedding_length=768, bert_path='bert-base-uncased', window=[7], classes=2, use_cuda=True):\n",
        "    super(myBert, self).__init__()\n",
        "    self.use_cuda = use_cuda\n",
        "    self.num_filter = len(window)\n",
        "\n",
        "    # Bert model\n",
        "    self.bert = BertModel.from_pretrained(bert_path)\n",
        "    \n",
        "    # Convolution layers\n",
        "    conv1, conv2, conv3, conv4 = [], [], [], []\n",
        "    for i in range(self.num_filter):\n",
        "      conv1.append(nn.Conv2d(1, 256, (window[i], embedding_length), stride=(1, 1), padding=(int((window[i] - 1) / 2), 0))) # out n*64*128*1\n",
        "      conv2.append(nn.Conv1d(256, 128, 3, stride=1, padding=1)) # out n*128*64\n",
        "      conv3.append(nn.Conv1d(128, 64, 3, stride=1, padding=1)) # out n*64*32\n",
        "      conv4.append(nn.Conv1d(64, 16, 1, stride=1, padding=0)) # out n*16*16\n",
        "    self.conv1, self.conv2 = nn.ModuleList(conv1), nn.ModuleList(conv2)\n",
        "    self.conv3, self.conv4 = nn.ModuleList(conv3), nn.ModuleList(conv4)\n",
        "    for i in range(self.num_filter):\n",
        "      init.kaiming_normal_(self.conv1[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv2[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv3[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv4[i].weight.data)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # LSTM layers\n",
        "    self.lstm1 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size) # out n*1*LSTM_Hidden_Size\n",
        "    self.lstm2 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size)\n",
        "    \n",
        "    # Attention\n",
        "    self.h1k1 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k1e1 = nn.Linear(64, 1)\n",
        "    self.h2k2 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k2e2 = nn.Linear(64, 1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(LSTM_Hidden_Size * 2 + 256 * self.num_filter, classes)\n",
        "    init.kaiming_normal_(self.fc.weight.data)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "    \n",
        "    self.simple_fc = nn.Linear(768, 2)\n",
        "    \n",
        "  def forward(self, inputs, mask):\n",
        "    # Get Features\n",
        "    inputs = self.bert(inputs, token_type_ids=None, attention_mask=mask, output_all_encoded_layers=False)[0]\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "        \n",
        "    # Go through Bi-LSTM\n",
        "    n = inputs.shape[0]\n",
        "    x0 = inputs.squeeze(1)\n",
        "    if self.use_cuda:\n",
        "      if not inputs.is_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "        x0 = x0.cuda()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "    else:\n",
        "      if inputs.is_cuda:\n",
        "        inputs = inputs.cpu()\n",
        "        x0 = x0.cpu()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "    \n",
        "    for i in range(x0.shape[1]):\n",
        "      hx1, cx1 = self.lstm1(x0[:, i, :], (hx1, cx1))\n",
        "      hxs1[:, i, :] = hx1\n",
        "      hx2, cx2 = self.lstm1(x0[:, x0.shape[1] - 1 - i, :], (hx2, cx2))\n",
        "      hxs2[:, i, :] = hx2\n",
        "      \n",
        "    k1 = self.h1k1(hxs1) # n*128*64\n",
        "    e1 = self.k1e1(k1).squeeze(2) # n*128*1 -> n*128\n",
        "    e1 = F.softmax(e1, dim=1).unsqueeze(1) # n*1*128\n",
        "    lstm1 = torch.matmul(e1, hxs1).squeeze(1)\n",
        "    k2 = self.h2k2(hxs2) # n*128*64\n",
        "    e2 = self.k2e2(k2).squeeze(2)\n",
        "    e2 = F.softmax(e2, dim=1).unsqueeze(1)\n",
        "    lstm2 = torch.matmul(e2, hxs2).squeeze(1)\n",
        "    \n",
        "    lstm_x = torch.cat((lstm1, lstm2), 1)\n",
        "    \n",
        "    # Go Through CNN\n",
        "    x = []\n",
        "    for i in range(self.num_filter):\n",
        "      x.append(self.conv1[i](inputs))\n",
        "      x[i] = x[i].squeeze(3)\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv2[i](x[i])\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv3[i](x[i])\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv4[i](x[i])\n",
        "      x[i] = x[i].view(n, -1)\n",
        "      x[i] = self.dropout(x[i])\n",
        "    cnn_x = torch.cat(x, 1)\n",
        "    \n",
        "    x1 = torch.cat((cnn_x, lstm_x), 1)\n",
        "    \n",
        "    x1 = self.dropout(self.fc(x1))\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KWTA4c88EvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class enBert(torch.nn.Module):\n",
        "  def __init__(self, embedding_length=768, bert_path='bert-base-uncased', window=[7], classes=2, use_cuda=True):\n",
        "    super(enBert, self).__init__()\n",
        "    self.use_cuda = use_cuda\n",
        "    self.num_filter = len(window)\n",
        "\n",
        "    # Bert model\n",
        "    self.bert = BertModel.from_pretrained(bert_path)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # LSTM layers\n",
        "    self.lstm1 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size) # out n*1*LSTM_Hidden_Size\n",
        "    self.lstm2 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size)\n",
        "    \n",
        "    # Attention\n",
        "    self.h1k1 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k1e1 = nn.Linear(64, 1)\n",
        "    self.h2k2 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k2e2 = nn.Linear(64, 1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(LSTM_Hidden_Size * 2, classes)\n",
        "    init.kaiming_normal_(self.fc.weight.data)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "    \n",
        "    self.simple_fc = nn.Linear(768, 2)\n",
        "    \n",
        "  def forward(self, inputs, mask):\n",
        "    # Get Features\n",
        "    inputs = self.bert(inputs, token_type_ids=None, attention_mask=mask, output_all_encoded_layers=False)[0]\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "        \n",
        "    # Go through Bi-LSTM\n",
        "    n = inputs.shape[0]\n",
        "    x0 = inputs.squeeze(1)\n",
        "    if self.use_cuda:\n",
        "      if not inputs.is_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "        x0 = x0.cuda()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "    else:\n",
        "      if inputs.is_cuda:\n",
        "        inputs = inputs.cpu()\n",
        "        x0 = x0.cpu()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "    \n",
        "    for i in range(x0.shape[1]):\n",
        "      hx1, cx1 = self.lstm1(x0[:, i, :], (hx1, cx1))\n",
        "      hxs1[:, i, :] = hx1\n",
        "      hx2, cx2 = self.lstm1(x0[:, x0.shape[1] - 1 - i, :], (hx2, cx2))\n",
        "      hxs2[:, i, :] = hx2\n",
        "      \n",
        "    k1 = self.h1k1(hxs1) # n*128*64\n",
        "    e1 = self.k1e1(k1).squeeze(2) # n*128*1 -> n*128\n",
        "    e1 = F.softmax(e1, dim=1).unsqueeze(1) # n*1*128\n",
        "    lstm1 = torch.matmul(e1, hxs1).squeeze(1)\n",
        "    k2 = self.h2k2(hxs2) # n*128*64\n",
        "    e2 = self.k2e2(k2).squeeze(2)\n",
        "    e2 = F.softmax(e2, dim=1).unsqueeze(1)\n",
        "    lstm2 = torch.matmul(e2, hxs2).squeeze(1)\n",
        "    lstm_x = torch.cat((lstm1, lstm2), 1)\n",
        "   \n",
        "    x1 = self.dropout(self.fc(lstm_x))\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unjfduMF03b6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "device = torch.device(\"cuda\")\n",
        "tag = 'cn' # en英文，cn中文\n",
        "en_checkpoint = 'en.pth'\n",
        "cn_checkpoint = 'cn.pth'\n",
        "testfile = 'task2_input_en.xml'\n",
        "testfile = 'task2_intput_cn.xml'\n",
        "bz = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXyuSGGg1FNR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tag == 'cn':\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "  myModel = myBert(embedding_length, 'bert-base-chinese', use_cuda=True)\n",
        "  myModel.to(device=device)\n",
        "  state = torch.load(cn_checkpoint)\n",
        "  myModel.load_state_dict(state['model_state'])\n",
        "else:\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  myModel = enBert(embedding_length, 'bert-base-uncased', use_cuda=True)\n",
        "  myModel.to(device=device)\n",
        "  state = torch.load(en_checkpoint)\n",
        "  myModel.load_state_dict(state['model_state'])\n",
        "\n",
        "myModel = myModel.eval()\n",
        "\n",
        "assert testfile != ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLkxQSBt1emL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xmltree = ET.parse(testfile)\n",
        "xmlroot = xmltree.getroot()\n",
        "lines = []\n",
        "for review in xmlroot:\n",
        "  s = review.text.strip()\n",
        "  s = ' '.join(s.split())\n",
        "  lines.append(s)\n",
        "tot = len(lines)\n",
        "test_index = torch.zeros((tot, 128)).long()\n",
        "test_mask = torch.ones((tot, 128)).long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vth6LWz1oBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(lines)):\n",
        "  s = lines[i]\n",
        "  tokenized_text = tokenizer.tokenize(s)[:128]\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  k = len(indexed_tokens)\n",
        "  while k < 128:\n",
        "    test_mask[i, k] = 0\n",
        "    indexed_tokens.append(0)\n",
        "    k += 1\n",
        "  test_index[i, :] = torch.Tensor(indexed_tokens).long()\n",
        "test_set = BertTestSet(test_index, test_mask)\n",
        "test_loader = train_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=16, shuffle=False, num_workers=8)\n",
        "test_res = torch.zeros(tot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz_iGMj3161J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for i, (index, mask) in enumerate(test_loader):\n",
        "  k = index.shape[0]\n",
        "  index = index.cuda()\n",
        "  mask = mask.cuda()\n",
        "  scores = myModel(index, mask)\n",
        "  ans = torch.argmax(scores, dim=1).long()\n",
        "  test_res[cnt:cnt + k] = ans\n",
        "  cnt += k\n",
        "    \n",
        "cnt = 0\n",
        "for review in xmlroot:\n",
        "  if test_res[cnt] == 0:\n",
        "    tmp = -1\n",
        "  else:\n",
        "    tmp = 1\n",
        "  review.set('polarity','%d' % tmp)\n",
        "  cnt += 1\n",
        "xmltree.write('oup_' + testfile, encoding=\"UTF-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}