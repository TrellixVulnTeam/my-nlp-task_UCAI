{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Foruck/sentiment-analysis-demo/blob/attention/bert_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6r0ZwikpHmYO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 准备工作"
      ]
    },
    {
      "metadata": {
        "id": "fxZi1hp_GZbK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 加载数据\n",
        "\n",
        "上传\n",
        "cn_train_data.h5\n",
        "en_train_data.h5\n",
        "cn_valid_data.h5\n",
        "en_valid_data.h5\n",
        "到自己的Google Drive\n"
      ]
    },
    {
      "metadata": {
        "id": "WVGVxS45EPaQ",
        "colab_type": "code",
        "outputId": "49623401-fd7a-4378-8513-497fd17454d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o3qrbzI6Gtuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 配置文件目录环境\n",
        "\n",
        "log：训练日志目录\n",
        "\n",
        "bst_model：最优checkpoint\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GAygoCb1KJLS",
        "colab_type": "code",
        "outputId": "e0d1de6f-47d4-4e8c-99eb-1804e6053d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install psmisc\n",
        "!rm -rf log\n",
        "!rm -rf bst_model\n",
        "!mkdir log\n",
        "!mkdir bst_model\n",
        "!cp /content/gdrive/My\\ Drive/cn_train_data.h5 cn_train_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/en_train_data.h5 en_train_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/cn_valid_data.h5 cn_valid_data.h5\n",
        "!cp /content/gdrive/My\\ Drive/en_valid_data.h5 en_valid_data.h5\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "psmisc is already the newest version (23.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "bst_model\t  cn_valid_data.h5  en_valid_data.h5  log\n",
            "cn_train_data.h5  en_train_data.h5  gdrive\t      sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wdc5tA4dHKJX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 配置python运行环境"
      ]
    },
    {
      "metadata": {
        "id": "C4vjhWx9R0CV",
        "colab_type": "code",
        "outputId": "f9c6d786-5f1f-4003-8c00-3b82df3fcc80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install torch torchvision\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "\n",
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Using cached https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60f88000 @  0x7f1fe8fde2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e3/217dfd0834a51418c602c96b110059c477260c7fee898542b100913947cf/Pillow-5.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.0 torch-1.0.0 torchvision-0.2.1\n",
            "Torch 1.0.0 CUDA 9.0.176\n",
            "Device: cuda:0\n",
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.6)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.67)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.67 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.67)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.1.13)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.67->boto3->pytorch_pretrained_bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fnVUgQZbHeEY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 开始"
      ]
    },
    {
      "metadata": {
        "id": "u6ZxFBtmHsXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 导入模块"
      ]
    },
    {
      "metadata": {
        "id": "1up16TvMNwHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_Cw9G6MHxI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 模型超参数设置\n",
        "\n",
        "embedding_length\n",
        "Sentence_Max_Length\n",
        "不可改动"
      ]
    },
    {
      "metadata": {
        "id": "rPzTLw2iNxoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_Hidden_Size = 256\n",
        "embedding_length = 768\n",
        "Sentence_Max_Length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmcepKJbH81_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 模型"
      ]
    },
    {
      "metadata": {
        "id": "MqvH0ISrEQi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class bertDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path):\n",
        "    with h5py.File(data_path, 'r') as f:\n",
        "      self.data = f['data'][:, :]\n",
        "      self.mask = f['mask'][:, :]\n",
        "      self.annot = f['annot'][:]\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.annot.shape[0]\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx, :], self.mask[idx, :], self.annot[idx]\n",
        "\n",
        "      \n",
        "class myBert(torch.nn.Module):\n",
        "  def __init__(self, embedding_length=768, bert_path='bert-base-chinese', window=[7], classes=2, use_cuda=True):\n",
        "    super(myBert, self).__init__()\n",
        "    self.use_cuda = use_cuda\n",
        "    self.num_filter = len(window)\n",
        "\n",
        "    # Bert model\n",
        "    self.bert = BertModel.from_pretrained(bert_path)\n",
        "    \n",
        "    # Convolution layers\n",
        "    conv1, conv2, conv3, conv4 = [], [], [], []\n",
        "    for i in range(self.num_filter):\n",
        "      conv1.append(nn.Conv2d(1, 256, (window[i], embedding_length), stride=(1, 1), padding=(int((window[i] - 1) / 2), 0))) # out n*64*128*1\n",
        "      conv2.append(nn.Conv1d(256, 128, 3, stride=1, padding=1)) # out n*128*64\n",
        "      conv3.append(nn.Conv1d(128, 64, 3, stride=1, padding=1)) # out n*64*32\n",
        "      conv4.append(nn.Conv1d(64, 16, 1, stride=1, padding=0)) # out n*16*16\n",
        "    self.conv1, self.conv2 = nn.ModuleList(conv1), nn.ModuleList(conv2)\n",
        "    self.conv3, self.conv4 = nn.ModuleList(conv3), nn.ModuleList(conv4)\n",
        "    for i in range(self.num_filter):\n",
        "      init.kaiming_normal_(self.conv1[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv2[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv3[i].weight.data)\n",
        "      init.kaiming_normal_(self.conv4[i].weight.data)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # LSTM layers\n",
        "    self.lstm1 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size) # out n*1*LSTM_Hidden_Size\n",
        "    self.lstm2 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size)\n",
        "    \n",
        "    # Attention\n",
        "    self.h1k1 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k1e1 = nn.Linear(64, 1)\n",
        "    self.h2k2 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k2e2 = nn.Linear(64, 1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(LSTM_Hidden_Size * 2 + 256 * self.num_filter, classes)\n",
        "    init.kaiming_normal_(self.fc.weight.data)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "    \n",
        "    self.simple_fc = nn.Linear(768, 2)\n",
        "    \n",
        "  def forward(self, inputs, mask):\n",
        "    # Get Features\n",
        "    inputs = self.bert(inputs, token_type_ids=None, attention_mask=mask, output_all_encoded_layers=False)[0]\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "        \n",
        "    # Go through Bi-LSTM\n",
        "    n = inputs.shape[0]\n",
        "    x0 = inputs.squeeze(1)\n",
        "    if self.use_cuda:\n",
        "      if not inputs.is_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "        x0 = x0.cuda()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "    else:\n",
        "      if inputs.is_cuda:\n",
        "        inputs = inputs.cpu()\n",
        "        x0 = x0.cpu()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "    \n",
        "    for i in range(x0.shape[1]):\n",
        "      hx1, cx1 = self.lstm1(x0[:, i, :], (hx1, cx1))\n",
        "      hxs1[:, i, :] = hx1\n",
        "      hx2, cx2 = self.lstm1(x0[:, x0.shape[1] - 1 - i, :], (hx2, cx2))\n",
        "      hxs2[:, i, :] = hx2\n",
        "      \n",
        "    k1 = self.h1k1(hxs1) # n*128*64\n",
        "    e1 = self.k1e1(k1).squeeze(2) # n*128*1 -> n*128\n",
        "    e1 = F.softmax(e1, dim=1).unsqueeze(1) # n*1*128\n",
        "    lstm1 = torch.matmul(e1, hxs1).squeeze(1)\n",
        "    k2 = self.h2k2(hxs2) # n*128*64\n",
        "    e2 = self.k2e2(k2).squeeze(2)\n",
        "    e2 = F.softmax(e2, dim=1).unsqueeze(1)\n",
        "    lstm2 = torch.matmul(e2, hxs2).squeeze(1)\n",
        "    lstm_x = torch.cat((lstm1, lstm2), 1)\n",
        "    \n",
        "    # Go Through CNN\n",
        "    x = []\n",
        "    for i in range(self.num_filter):\n",
        "      x.append(self.conv1[i](inputs))\n",
        "      x[i] = x[i].squeeze(3)\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv2[i](x[i])\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv3[i](x[i])\n",
        "      x[i] = F.relu(F.max_pool1d(x[i], kernel_size=2, stride=2))\n",
        "      x[i] = self.conv4[i](x[i])\n",
        "      x[i] = x[i].view(n, -1)\n",
        "      x[i] = self.dropout(x[i])\n",
        "    cnn_x = torch.cat(x, 1)\n",
        "    \n",
        "    x1 = torch.cat((cnn_x, lstm_x), 1)\n",
        "    \n",
        "    x1 = self.dropout(self.fc(x1))\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CD0dThkw1Wgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class enBert(torch.nn.Module):\n",
        "  def __init__(self, embedding_length=768, bert_path='bert-base-uncased', window=[7], classes=2, use_cuda=True):\n",
        "    super(enBert, self).__init__()\n",
        "    self.use_cuda = use_cuda\n",
        "    self.num_filter = len(window)\n",
        "\n",
        "    # Bert model\n",
        "    self.bert = BertModel.from_pretrained(bert_path)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    # LSTM layers\n",
        "    self.lstm1 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size) # out n*1*LSTM_Hidden_Size\n",
        "    self.lstm2 = nn.LSTMCell(embedding_length, LSTM_Hidden_Size)\n",
        "    \n",
        "    # Attention\n",
        "    self.h1k1 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k1e1 = nn.Linear(64, 1)\n",
        "    self.h2k2 = nn.Linear(LSTM_Hidden_Size, 64)\n",
        "    self.k2e2 = nn.Linear(64, 1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(LSTM_Hidden_Size * 2, classes)\n",
        "    init.kaiming_normal_(self.fc.weight.data)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "    \n",
        "    self.simple_fc = nn.Linear(768, 2)\n",
        "    \n",
        "  def forward(self, inputs, mask):\n",
        "    # Get Features\n",
        "    inputs = self.bert(inputs, token_type_ids=None, attention_mask=mask, output_all_encoded_layers=False)[0]\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "        \n",
        "    # Go through Bi-LSTM\n",
        "    n = inputs.shape[0]\n",
        "    x0 = inputs.squeeze(1)\n",
        "    if self.use_cuda:\n",
        "      if not inputs.is_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "        x0 = x0.cuda()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size).cuda()\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size)).cuda()\n",
        "    else:\n",
        "      if inputs.is_cuda:\n",
        "        inputs = inputs.cpu()\n",
        "        x0 = x0.cpu()\n",
        "      cx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx1 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      cx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hx2 = torch.zeros(n, LSTM_Hidden_Size)\n",
        "      hxs1 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "      hxs2 = torch.zeros((n, x0.shape[1], LSTM_Hidden_Size))\n",
        "    \n",
        "    for i in range(x0.shape[1]):\n",
        "      hx1, cx1 = self.lstm1(x0[:, i, :], (hx1, cx1))\n",
        "      hxs1[:, i, :] = hx1\n",
        "      hx2, cx2 = self.lstm1(x0[:, x0.shape[1] - 1 - i, :], (hx2, cx2))\n",
        "      hxs2[:, i, :] = hx2\n",
        "      \n",
        "    k1 = self.h1k1(hxs1) # n*128*64\n",
        "    e1 = self.k1e1(k1).squeeze(2) # n*128*1 -> n*128\n",
        "    e1 = F.softmax(e1, dim=1).unsqueeze(1) # n*1*128\n",
        "    lstm1 = torch.matmul(e1, hxs1).squeeze(1)\n",
        "    k2 = self.h2k2(hxs2) # n*128*64\n",
        "    e2 = self.k2e2(k2).squeeze(2)\n",
        "    e2 = F.softmax(e2, dim=1).unsqueeze(1)\n",
        "    lstm2 = torch.matmul(e2, hxs2).squeeze(1)\n",
        "    lstm_x = torch.cat((lstm1, lstm2), 1)\n",
        "   \n",
        "    x1 = self.dropout(self.fc(lstm_x))\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RameSu2AIAf3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 训练与测试函数"
      ]
    },
    {
      "metadata": {
        "id": "SUqQ3pTgN_1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, logger, epoch=0, print_every=100):\n",
        "  model = model.train()\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  all_loss, all_accuracy = 0.0, 0.0\n",
        "  hit, cnt = 0, 0\n",
        "  for i, (x, mask, target) in enumerate(train_loader):\n",
        "    x = x.cuda().long()\n",
        "    mask = mask.cuda().long()\n",
        "    target = target.cuda().long()\n",
        "    target = torch.clamp(target, min=0, max=1)\n",
        "    \n",
        "    scores = model(x, mask)\n",
        "    loss = loss_fn(scores, target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    pred = torch.argmax(scores, dim=1)\n",
        "    hit = torch.sum(target == pred)\n",
        "    accuracy = float(hit) / int(x.shape[0])\n",
        "    if i % print_every == 0:\n",
        "      logger.info('Epoch %d, Iter %d, loss=%.4f, acc=%.4f' % (epoch, i, loss, accuracy))\n",
        "      print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, Iter %d, loss=%.4f, acc=%.4f' % (epoch, i, loss, accuracy))\n",
        "    \n",
        "    all_loss += float(loss) * int(x.shape[0])\n",
        "    all_accuracy += float(hit)\n",
        "    cnt += int(x.shape[0])\n",
        "    \n",
        "  all_loss /= float(cnt)\n",
        "  all_accuracy /= float(cnt)\n",
        "  logger.info('Epoch %d, train_loss=%.4f, train_accuracy=%.4f' % (epoch, all_loss, all_accuracy))\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, train_loss=%.4f, train_accuracy=%.4f' % (epoch, all_loss, all_accuracy))\n",
        "  return model, optimizer\n",
        "\n",
        "def evaluate(model, valid_loader, logger, epoch=0, print_every=100):\n",
        "  model = model.eval()\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  all_loss = 0.0\n",
        "  hit, tot = 0, 0\n",
        "  for i, (x, mask, target) in enumerate(valid_loader):\n",
        "    x = x.cuda().long()\n",
        "    mask = mask.cuda().long()\n",
        "    target = target.cuda().long()\n",
        "    target = torch.clamp(target, min=0, max=1)\n",
        "    \n",
        "    scores = model(x, mask)\n",
        "    loss = loss_fn(scores, target)\n",
        "    pred = torch.argmax(scores, dim=1)\n",
        "    \n",
        "    hit += float(torch.sum(target == pred))\n",
        "    all_loss += float(loss) * int(x.shape[0])\n",
        "    tot += int(x.shape[0])\n",
        "  \n",
        "  all_loss /= tot\n",
        "  accuracy = float(hit) / tot\n",
        "  logger.info('Epoch %d, valid_loss=%.4f, valid_accuracy=%.4f' % (epoch, all_loss, accuracy))\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S \", time.localtime()) + 'Epoch %d, valid_loss=%.4f, valid_accuracy=%.4f' % (epoch, all_loss, accuracy))\n",
        "  return model, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMEkaDU8IEhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 查看显存使用状况"
      ]
    },
    {
      "metadata": {
        "id": "DZY_IjiVO4Zt",
        "colab_type": "code",
        "outputId": "c29cebb1-5ae7-475c-886b-e98ba6839f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan  2 08:12:37 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    67W / 149W |   7848MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4iMOZxY8IHmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 训练参数设置"
      ]
    },
    {
      "metadata": {
        "id": "hqUNGt4SOTc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "device = torch.device(\"cuda\")\n",
        "end_epoch = 60\n",
        "tag = 'cn' # en英文，cn中文\n",
        "lr = 1e-3 #可以调节\n",
        "# checkpoint = 'gdrive/My Drive/cn_attention.pth'\n",
        "# checkpoint = 'gdrive/My Drive/en_5e-05_2018-12-23_04-48-53.pth'\n",
        "checkpoint = ''\n",
        "fine_tune = 5e-5\n",
        "bz = 30\n",
        "warmup_proportion = 0.1\n",
        "bst_acc = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Q6V9yWkIKi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 数据集、模型加载"
      ]
    },
    {
      "metadata": {
        "id": "4tvUPCngPTQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tag == 'cn':\n",
        "  train_set = bertDataset('cn_train_data.h5')\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bz, shuffle=True, num_workers=8)\n",
        "  valid_set = bertDataset('cn_valid_data.h5')\n",
        "  valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=int(bz / 2), shuffle=True, num_workers=8)\n",
        "  myModel = enBert(embedding_length, 'bert-base-chinese', use_cuda=True)\n",
        "elif tag == 'en':\n",
        "  train_set = bertDataset('en_train_data.h5')\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bz, shuffle=True, num_workers=8)\n",
        "  valid_set = bertDataset('en_valid_data.h5')\n",
        "  valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=int(bz / 2), shuffle=True, num_workers=8)\n",
        "  myModel = enBert(embedding_length, use_cuda=True)\n",
        "  \n",
        "\n",
        "myModel = myModel.cuda()\n",
        "ignored_params = list(map(id, myModel.bert.parameters()))\n",
        "base_params = filter(lambda p: id(p) not in ignored_params, myModel.parameters())\n",
        "optimizer_grouped_parameters = [{'params': base_params, 'weight_decay': 0.01}, {'params': myModel.bert.parameters(), 'weight_decay': 0.0, 'lr': fine_tune}]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=lr, warmup=warmup_proportion, t_total=train_set.__len__())\n",
        "  \n",
        "if checkpoint != '':\n",
        "  state = torch.load(checkpoint)\n",
        "  myModel.load_state_dict(state['model_state'])\n",
        "  optimizer.load_state_dict(state['optim_state'])\n",
        "  epoch = state['epoch']\n",
        "  bst_acc = state['acc']\n",
        "else:\n",
        "  epoch = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gecyiAAgIN-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### log、checkpoint存储目录设置"
      ]
    },
    {
      "metadata": {
        "id": "7ZpYW4W1RAqs",
        "colab_type": "code",
        "outputId": "6f00a147-1a9e-4ba1-a23b-c54d7d8ccbbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "job_name =  '_'.join([tag, str(lr), datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')])\n",
        "save_path = 'bst_model/' + job_name + '.pth'\n",
        "log_path = 'log/' + job_name + '.log'\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(level = logging.INFO)\n",
        "handler = logging.FileHandler(log_path)\n",
        "handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "logger.info(\"Start print log\")\n",
        "print(log_path)\n",
        "print(epoch)\n",
        "print(bst_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log/cn_0.001_2019-01-02_08-12-47.log\n",
            "0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKtl0HAuIT6e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 开始训练"
      ]
    },
    {
      "metadata": {
        "id": "wAX-He7RRIxE",
        "colab_type": "code",
        "outputId": "606fff36-5442-4184-da85-bc0497eb6be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5309
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch, end_epoch):\n",
        "  logger.info('=> Epoch %d, lr = %0.6f <=' % (i, lr))\n",
        "  myModel, optimizer = train(myModel, train_loader, optimizer, logger, epoch=i, print_every=50)\n",
        "  myModel, accuracy = evaluate(myModel, valid_loader, logger, epoch=i)\n",
        "  # break\n",
        "  \n",
        "  if (accuracy > bst_acc):\n",
        "    bst_acc = accuracy\n",
        "    state = {'model_state': myModel.state_dict(), 'epoch': i, 'optim_state': optimizer.state_dict(), 'acc': bst_acc}\n",
        "    torch.save(state, save_path)\n",
        "  \n",
        "logger.info(\"Finish\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-01-02 08:12:50 Epoch 0, Iter 0, loss=0.8658, acc=0.4667\n",
            "2019-01-02 08:14:25 Epoch 0, Iter 50, loss=0.6304, acc=0.6667\n",
            "2019-01-02 08:16:00 Epoch 0, Iter 100, loss=0.4329, acc=0.7667\n",
            "2019-01-02 08:17:35 Epoch 0, Iter 150, loss=0.5266, acc=0.7667\n",
            "2019-01-02 08:19:11 Epoch 0, Iter 200, loss=0.4583, acc=0.8667\n",
            "2019-01-02 08:20:46 Epoch 0, Iter 250, loss=0.4582, acc=0.7333\n",
            "2019-01-02 08:22:21 Epoch 0, Iter 300, loss=0.2874, acc=0.8000\n",
            "2019-01-02 08:23:56 Epoch 0, Iter 350, loss=0.2923, acc=0.8667\n",
            "2019-01-02 08:25:31 Epoch 0, Iter 400, loss=0.6600, acc=0.5333\n",
            "2019-01-02 08:27:07 Epoch 0, Iter 450, loss=0.4789, acc=0.7333\n",
            "2019-01-02 08:28:42 Epoch 0, Iter 500, loss=0.6297, acc=0.7667\n",
            "2019-01-02 08:30:17 Epoch 0, Iter 550, loss=0.4269, acc=0.8333\n",
            "2019-01-02 08:31:57 Epoch 0, Iter 600, loss=0.3250, acc=0.8000\n",
            "2019-01-02 08:33:33 Epoch 0, Iter 650, loss=0.4460, acc=0.6667\n",
            "2019-01-02 08:35:08 Epoch 0, Iter 700, loss=0.5740, acc=0.6667\n",
            "2019-01-02 08:36:43 Epoch 0, Iter 750, loss=0.3808, acc=0.8333\n",
            "2019-01-02 08:38:18 Epoch 0, Iter 800, loss=0.5614, acc=0.7333\n",
            "2019-01-02 08:39:53 Epoch 0, Iter 850, loss=0.3475, acc=0.8333\n",
            "2019-01-02 08:41:28 Epoch 0, Iter 900, loss=0.3559, acc=0.8000\n",
            "2019-01-02 08:42:11 Epoch 0, train_loss=0.4740, train_accuracy=0.7529\n",
            "2019-01-02 08:44:44 Epoch 0, valid_loss=0.3038, valid_accuracy=0.8789\n",
            "2019-01-02 08:44:54 Epoch 1, Iter 0, loss=0.5334, acc=0.7333\n",
            "2019-01-02 08:46:29 Epoch 1, Iter 50, loss=0.3975, acc=0.8000\n",
            "2019-01-02 08:48:04 Epoch 1, Iter 100, loss=0.3792, acc=0.8000\n",
            "2019-01-02 08:49:39 Epoch 1, Iter 150, loss=0.3846, acc=0.8000\n",
            "2019-01-02 08:51:14 Epoch 1, Iter 200, loss=0.4490, acc=0.8000\n",
            "2019-01-02 08:52:49 Epoch 1, Iter 250, loss=0.3348, acc=0.7667\n",
            "2019-01-02 08:54:25 Epoch 1, Iter 300, loss=0.4167, acc=0.6667\n",
            "2019-01-02 08:56:00 Epoch 1, Iter 350, loss=0.7621, acc=0.7000\n",
            "2019-01-02 08:57:35 Epoch 1, Iter 400, loss=0.4268, acc=0.7000\n",
            "2019-01-02 08:59:10 Epoch 1, Iter 450, loss=0.4275, acc=0.8333\n",
            "2019-01-02 09:00:45 Epoch 1, Iter 500, loss=0.3731, acc=0.7667\n",
            "2019-01-02 09:02:20 Epoch 1, Iter 550, loss=0.3344, acc=0.9000\n",
            "2019-01-02 09:03:56 Epoch 1, Iter 600, loss=0.8315, acc=0.7000\n",
            "2019-01-02 09:05:31 Epoch 1, Iter 650, loss=0.5443, acc=0.7000\n",
            "2019-01-02 09:07:06 Epoch 1, Iter 700, loss=0.4124, acc=0.7333\n",
            "2019-01-02 09:08:41 Epoch 1, Iter 750, loss=0.4412, acc=0.6667\n",
            "2019-01-02 09:10:16 Epoch 1, Iter 800, loss=0.4590, acc=0.8333\n",
            "2019-01-02 09:11:51 Epoch 1, Iter 850, loss=0.3726, acc=0.7333\n",
            "2019-01-02 09:13:27 Epoch 1, Iter 900, loss=0.2009, acc=0.9667\n",
            "2019-01-02 09:14:10 Epoch 1, train_loss=0.4172, train_accuracy=0.7901\n",
            "2019-01-02 09:16:44 Epoch 1, valid_loss=0.3130, valid_accuracy=0.8819\n",
            "2019-01-02 09:16:53 Epoch 2, Iter 0, loss=0.3609, acc=0.8000\n",
            "2019-01-02 09:18:28 Epoch 2, Iter 50, loss=0.2096, acc=0.9667\n",
            "2019-01-02 09:20:04 Epoch 2, Iter 100, loss=0.4741, acc=0.8333\n",
            "2019-01-02 09:21:39 Epoch 2, Iter 150, loss=0.4061, acc=0.8667\n",
            "2019-01-02 09:23:14 Epoch 2, Iter 200, loss=0.3840, acc=0.7667\n",
            "2019-01-02 09:24:49 Epoch 2, Iter 250, loss=0.4359, acc=0.7667\n",
            "2019-01-02 09:26:24 Epoch 2, Iter 300, loss=0.4136, acc=0.8667\n",
            "2019-01-02 09:27:59 Epoch 2, Iter 350, loss=0.2177, acc=0.9000\n",
            "2019-01-02 09:29:35 Epoch 2, Iter 400, loss=0.5971, acc=0.8333\n",
            "2019-01-02 09:31:10 Epoch 2, Iter 450, loss=0.4114, acc=0.8000\n",
            "2019-01-02 09:32:45 Epoch 2, Iter 500, loss=0.3285, acc=0.8333\n",
            "2019-01-02 09:34:21 Epoch 2, Iter 550, loss=0.3881, acc=0.7667\n",
            "2019-01-02 09:35:56 Epoch 2, Iter 600, loss=0.4692, acc=0.8000\n",
            "2019-01-02 09:37:31 Epoch 2, Iter 650, loss=0.3860, acc=0.8667\n",
            "2019-01-02 09:39:06 Epoch 2, Iter 700, loss=0.3559, acc=0.7667\n",
            "2019-01-02 09:40:41 Epoch 2, Iter 750, loss=0.3960, acc=0.7667\n",
            "2019-01-02 09:42:17 Epoch 2, Iter 800, loss=0.5551, acc=0.7667\n",
            "2019-01-02 09:43:52 Epoch 2, Iter 850, loss=0.5213, acc=0.9000\n",
            "2019-01-02 09:45:27 Epoch 2, Iter 900, loss=0.4557, acc=0.7333\n",
            "2019-01-02 09:46:11 Epoch 2, train_loss=0.3998, train_accuracy=0.7996\n",
            "2019-01-02 09:48:44 Epoch 2, valid_loss=0.3297, valid_accuracy=0.8645\n",
            "2019-01-02 09:48:47 Epoch 3, Iter 0, loss=0.5986, acc=0.7667\n",
            "2019-01-02 09:50:22 Epoch 3, Iter 50, loss=0.3590, acc=0.8333\n",
            "2019-01-02 09:51:57 Epoch 3, Iter 100, loss=0.5355, acc=0.7000\n",
            "2019-01-02 09:53:32 Epoch 3, Iter 150, loss=0.1646, acc=0.9667\n",
            "2019-01-02 09:55:07 Epoch 3, Iter 200, loss=0.2583, acc=0.8000\n",
            "2019-01-02 09:56:42 Epoch 3, Iter 250, loss=0.3959, acc=0.7667\n",
            "2019-01-02 09:58:18 Epoch 3, Iter 300, loss=0.3653, acc=0.8333\n",
            "2019-01-02 09:59:53 Epoch 3, Iter 350, loss=0.3616, acc=0.7667\n",
            "2019-01-02 10:01:28 Epoch 3, Iter 400, loss=0.3841, acc=0.8333\n",
            "2019-01-02 10:03:04 Epoch 3, Iter 450, loss=0.2993, acc=0.9000\n",
            "2019-01-02 10:04:39 Epoch 3, Iter 500, loss=0.3466, acc=0.8333\n",
            "2019-01-02 10:06:14 Epoch 3, Iter 550, loss=0.4252, acc=0.8667\n",
            "2019-01-02 10:07:49 Epoch 3, Iter 600, loss=0.4855, acc=0.7667\n",
            "2019-01-02 10:09:24 Epoch 3, Iter 650, loss=0.4519, acc=0.8000\n",
            "2019-01-02 10:10:59 Epoch 3, Iter 700, loss=0.5376, acc=0.7000\n",
            "2019-01-02 10:12:35 Epoch 3, Iter 750, loss=0.4666, acc=0.7667\n",
            "2019-01-02 10:14:10 Epoch 3, Iter 800, loss=0.4216, acc=0.8000\n",
            "2019-01-02 10:15:45 Epoch 3, Iter 850, loss=0.3337, acc=0.8667\n",
            "2019-01-02 10:17:20 Epoch 3, Iter 900, loss=0.4047, acc=0.7667\n",
            "2019-01-02 10:18:04 Epoch 3, train_loss=0.3776, train_accuracy=0.8098\n",
            "2019-01-02 10:20:37 Epoch 3, valid_loss=0.2948, valid_accuracy=0.8915\n",
            "2019-01-02 10:20:47 Epoch 4, Iter 0, loss=0.3879, acc=0.7667\n",
            "2019-01-02 10:22:22 Epoch 4, Iter 50, loss=0.3187, acc=0.8000\n",
            "2019-01-02 10:23:58 Epoch 4, Iter 100, loss=0.2800, acc=0.8333\n",
            "2019-01-02 10:25:33 Epoch 4, Iter 150, loss=0.3505, acc=0.7667\n",
            "2019-01-02 10:27:08 Epoch 4, Iter 200, loss=0.6258, acc=0.9000\n",
            "2019-01-02 10:28:43 Epoch 4, Iter 250, loss=0.2764, acc=0.8667\n",
            "2019-01-02 10:30:18 Epoch 4, Iter 300, loss=0.3528, acc=0.7667\n",
            "2019-01-02 10:31:53 Epoch 4, Iter 350, loss=0.3649, acc=0.8667\n",
            "2019-01-02 10:33:29 Epoch 4, Iter 400, loss=0.2445, acc=0.9000\n",
            "2019-01-02 10:35:04 Epoch 4, Iter 450, loss=0.3614, acc=0.8000\n",
            "2019-01-02 10:36:39 Epoch 4, Iter 500, loss=0.3274, acc=0.8000\n",
            "2019-01-02 10:38:14 Epoch 4, Iter 550, loss=0.2953, acc=0.8667\n",
            "2019-01-02 10:39:49 Epoch 4, Iter 600, loss=0.2539, acc=0.8333\n",
            "2019-01-02 10:41:25 Epoch 4, Iter 650, loss=0.3200, acc=0.7667\n",
            "2019-01-02 10:43:00 Epoch 4, Iter 700, loss=0.4738, acc=0.6667\n",
            "2019-01-02 10:44:35 Epoch 4, Iter 750, loss=0.3176, acc=0.7667\n",
            "2019-01-02 10:46:10 Epoch 4, Iter 800, loss=0.4255, acc=0.8000\n",
            "2019-01-02 10:47:45 Epoch 4, Iter 850, loss=0.3402, acc=0.8333\n",
            "2019-01-02 10:49:20 Epoch 4, Iter 900, loss=0.3270, acc=0.8333\n",
            "2019-01-02 10:50:04 Epoch 4, train_loss=0.3617, train_accuracy=0.8169\n",
            "2019-01-02 10:52:38 Epoch 4, valid_loss=0.3133, valid_accuracy=0.8815\n",
            "2019-01-02 10:52:40 Epoch 5, Iter 0, loss=0.3161, acc=0.8000\n",
            "2019-01-02 10:54:15 Epoch 5, Iter 50, loss=0.1931, acc=0.9333\n",
            "2019-01-02 10:55:50 Epoch 5, Iter 100, loss=0.5385, acc=0.7667\n",
            "2019-01-02 10:57:25 Epoch 5, Iter 150, loss=0.3753, acc=0.8667\n",
            "2019-01-02 10:59:01 Epoch 5, Iter 200, loss=0.3112, acc=0.7667\n",
            "2019-01-02 11:00:36 Epoch 5, Iter 250, loss=0.3904, acc=0.7333\n",
            "2019-01-02 11:02:11 Epoch 5, Iter 300, loss=0.3086, acc=0.7667\n",
            "2019-01-02 11:03:46 Epoch 5, Iter 350, loss=0.3658, acc=0.8000\n",
            "2019-01-02 11:05:21 Epoch 5, Iter 400, loss=0.3615, acc=0.8000\n",
            "2019-01-02 11:06:56 Epoch 5, Iter 450, loss=0.3437, acc=0.8667\n",
            "2019-01-02 11:08:31 Epoch 5, Iter 500, loss=0.3766, acc=0.7000\n",
            "2019-01-02 11:10:06 Epoch 5, Iter 550, loss=0.1866, acc=0.9667\n",
            "2019-01-02 11:11:42 Epoch 5, Iter 600, loss=0.2935, acc=0.8000\n",
            "2019-01-02 11:13:17 Epoch 5, Iter 650, loss=0.1991, acc=0.9000\n",
            "2019-01-02 11:14:52 Epoch 5, Iter 700, loss=0.2855, acc=0.8000\n",
            "2019-01-02 11:16:27 Epoch 5, Iter 750, loss=0.2521, acc=0.8667\n",
            "2019-01-02 11:18:02 Epoch 5, Iter 800, loss=0.3026, acc=0.8333\n",
            "2019-01-02 11:19:37 Epoch 5, Iter 850, loss=0.4579, acc=0.8000\n",
            "2019-01-02 11:21:12 Epoch 5, Iter 900, loss=0.2862, acc=0.8667\n",
            "2019-01-02 11:21:56 Epoch 5, train_loss=0.3356, train_accuracy=0.8302\n",
            "2019-01-02 11:24:30 Epoch 5, valid_loss=0.3058, valid_accuracy=0.8827\n",
            "2019-01-02 11:24:32 Epoch 6, Iter 0, loss=0.2392, acc=0.9000\n",
            "2019-01-02 11:26:07 Epoch 6, Iter 50, loss=0.2145, acc=0.9000\n",
            "2019-01-02 11:27:42 Epoch 6, Iter 100, loss=0.3346, acc=0.7667\n",
            "2019-01-02 11:29:18 Epoch 6, Iter 150, loss=0.6552, acc=0.7000\n",
            "2019-01-02 11:30:53 Epoch 6, Iter 200, loss=0.2849, acc=0.8000\n",
            "2019-01-02 11:32:28 Epoch 6, Iter 250, loss=0.3279, acc=0.8667\n",
            "2019-01-02 11:34:03 Epoch 6, Iter 300, loss=0.2615, acc=0.8333\n",
            "2019-01-02 11:35:38 Epoch 6, Iter 350, loss=0.1875, acc=0.8333\n",
            "2019-01-02 11:37:14 Epoch 6, Iter 400, loss=0.3684, acc=0.8000\n",
            "2019-01-02 11:38:49 Epoch 6, Iter 450, loss=0.5038, acc=0.6667\n",
            "2019-01-02 11:40:24 Epoch 6, Iter 500, loss=0.1437, acc=0.9333\n",
            "2019-01-02 11:41:59 Epoch 6, Iter 550, loss=0.2704, acc=0.7333\n",
            "2019-01-02 11:43:35 Epoch 6, Iter 600, loss=0.6781, acc=0.7000\n",
            "2019-01-02 11:45:10 Epoch 6, Iter 650, loss=0.3592, acc=0.7000\n",
            "2019-01-02 11:46:45 Epoch 6, Iter 700, loss=0.4488, acc=0.6667\n",
            "2019-01-02 11:48:20 Epoch 6, Iter 750, loss=0.2942, acc=0.8000\n",
            "2019-01-02 11:49:55 Epoch 6, Iter 800, loss=0.3000, acc=0.9000\n",
            "2019-01-02 11:51:31 Epoch 6, Iter 850, loss=0.2081, acc=0.8333\n",
            "2019-01-02 11:53:06 Epoch 6, Iter 900, loss=0.2898, acc=0.8000\n",
            "2019-01-02 11:53:49 Epoch 6, train_loss=0.3231, train_accuracy=0.8356\n",
            "2019-01-02 11:56:23 Epoch 6, valid_loss=0.3327, valid_accuracy=0.8831\n",
            "2019-01-02 11:56:26 Epoch 7, Iter 0, loss=0.3595, acc=0.8333\n",
            "2019-01-02 11:58:01 Epoch 7, Iter 50, loss=0.2399, acc=0.9333\n",
            "2019-01-02 11:59:36 Epoch 7, Iter 100, loss=0.1902, acc=0.9333\n",
            "2019-01-02 12:01:11 Epoch 7, Iter 150, loss=0.3994, acc=0.8667\n",
            "2019-01-02 12:02:46 Epoch 7, Iter 200, loss=0.2638, acc=0.8000\n",
            "2019-01-02 12:04:22 Epoch 7, Iter 250, loss=0.1593, acc=0.8333\n",
            "2019-01-02 12:05:57 Epoch 7, Iter 300, loss=0.2579, acc=0.8667\n",
            "2019-01-02 12:07:32 Epoch 7, Iter 350, loss=0.6333, acc=0.7333\n",
            "2019-01-02 12:09:07 Epoch 7, Iter 400, loss=0.2756, acc=0.8667\n",
            "2019-01-02 12:10:42 Epoch 7, Iter 450, loss=0.3985, acc=0.8333\n",
            "2019-01-02 12:12:18 Epoch 7, Iter 500, loss=0.1797, acc=0.8667\n",
            "2019-01-02 12:13:53 Epoch 7, Iter 550, loss=0.3087, acc=0.8333\n",
            "2019-01-02 12:15:28 Epoch 7, Iter 600, loss=0.1957, acc=0.8333\n",
            "2019-01-02 12:17:03 Epoch 7, Iter 650, loss=0.2708, acc=0.9000\n",
            "2019-01-02 12:18:39 Epoch 7, Iter 700, loss=0.5980, acc=0.7333\n",
            "2019-01-02 12:20:14 Epoch 7, Iter 750, loss=0.1518, acc=0.9333\n",
            "2019-01-02 12:21:49 Epoch 7, Iter 800, loss=0.5648, acc=0.7000\n",
            "2019-01-02 12:23:24 Epoch 7, Iter 850, loss=0.3945, acc=0.8667\n",
            "2019-01-02 12:25:00 Epoch 7, Iter 900, loss=0.3103, acc=0.7333\n",
            "2019-01-02 12:25:43 Epoch 7, train_loss=0.3114, train_accuracy=0.8411\n",
            "2019-01-02 12:28:17 Epoch 7, valid_loss=0.3620, valid_accuracy=0.8816\n",
            "2019-01-02 12:28:19 Epoch 8, Iter 0, loss=0.5817, acc=0.7667\n",
            "2019-01-02 12:29:55 Epoch 8, Iter 50, loss=0.2513, acc=0.8000\n",
            "2019-01-02 12:31:30 Epoch 8, Iter 100, loss=0.4993, acc=0.7667\n",
            "2019-01-02 12:33:05 Epoch 8, Iter 150, loss=0.1884, acc=0.9000\n",
            "2019-01-02 12:34:40 Epoch 8, Iter 200, loss=0.4084, acc=0.8667\n",
            "2019-01-02 12:36:16 Epoch 8, Iter 250, loss=0.5273, acc=0.7667\n",
            "2019-01-02 12:37:51 Epoch 8, Iter 300, loss=0.2183, acc=0.8333\n",
            "2019-01-02 12:39:26 Epoch 8, Iter 350, loss=0.5244, acc=0.7000\n",
            "2019-01-02 12:41:02 Epoch 8, Iter 400, loss=0.3645, acc=0.9000\n",
            "2019-01-02 12:42:37 Epoch 8, Iter 450, loss=0.1846, acc=0.8000\n",
            "2019-01-02 12:44:12 Epoch 8, Iter 500, loss=0.4788, acc=0.7667\n",
            "2019-01-02 12:45:47 Epoch 8, Iter 550, loss=0.3950, acc=0.8333\n",
            "2019-01-02 12:47:23 Epoch 8, Iter 600, loss=0.1334, acc=0.8667\n",
            "2019-01-02 12:48:58 Epoch 8, Iter 650, loss=0.3626, acc=0.8000\n",
            "2019-01-02 12:50:33 Epoch 8, Iter 700, loss=0.4581, acc=0.8667\n",
            "2019-01-02 12:52:09 Epoch 8, Iter 750, loss=0.2953, acc=0.8333\n",
            "2019-01-02 12:53:44 Epoch 8, Iter 800, loss=0.3174, acc=0.9000\n",
            "2019-01-02 12:55:19 Epoch 8, Iter 850, loss=0.1758, acc=0.8333\n",
            "2019-01-02 12:56:55 Epoch 8, Iter 900, loss=0.2996, acc=0.9000\n",
            "2019-01-02 12:57:38 Epoch 8, train_loss=0.3085, train_accuracy=0.8437\n",
            "2019-01-02 13:00:12 Epoch 8, valid_loss=0.3625, valid_accuracy=0.8779\n",
            "2019-01-02 13:00:15 Epoch 9, Iter 0, loss=0.4786, acc=0.7333\n",
            "2019-01-02 13:01:50 Epoch 9, Iter 50, loss=0.3137, acc=0.8667\n",
            "2019-01-02 13:03:25 Epoch 9, Iter 100, loss=0.3146, acc=0.7667\n",
            "2019-01-02 13:05:00 Epoch 9, Iter 150, loss=0.1914, acc=0.8667\n",
            "2019-01-02 13:06:36 Epoch 9, Iter 200, loss=0.3664, acc=0.7333\n",
            "2019-01-02 13:08:11 Epoch 9, Iter 250, loss=0.2229, acc=0.9333\n",
            "2019-01-02 13:09:46 Epoch 9, Iter 300, loss=0.3571, acc=0.8667\n",
            "2019-01-02 13:11:22 Epoch 9, Iter 350, loss=0.2829, acc=0.9333\n",
            "2019-01-02 13:12:57 Epoch 9, Iter 400, loss=0.2042, acc=0.9000\n",
            "2019-01-02 13:14:32 Epoch 9, Iter 450, loss=0.1386, acc=0.9333\n",
            "2019-01-02 13:16:08 Epoch 9, Iter 500, loss=0.2378, acc=0.8000\n",
            "2019-01-02 13:17:43 Epoch 9, Iter 550, loss=0.3169, acc=0.7000\n",
            "2019-01-02 13:19:18 Epoch 9, Iter 600, loss=0.4053, acc=0.8333\n",
            "2019-01-02 13:20:53 Epoch 9, Iter 650, loss=0.1290, acc=0.9333\n",
            "2019-01-02 13:22:29 Epoch 9, Iter 700, loss=0.1527, acc=0.9333\n",
            "2019-01-02 13:24:04 Epoch 9, Iter 750, loss=0.2588, acc=0.9000\n",
            "2019-01-02 13:25:39 Epoch 9, Iter 800, loss=0.2093, acc=0.9000\n",
            "2019-01-02 13:27:15 Epoch 9, Iter 850, loss=0.2968, acc=0.8667\n",
            "2019-01-02 13:28:50 Epoch 9, Iter 900, loss=0.2791, acc=0.8667\n",
            "2019-01-02 13:29:33 Epoch 9, train_loss=0.3007, train_accuracy=0.8436\n",
            "2019-01-02 13:32:07 Epoch 9, valid_loss=0.3290, valid_accuracy=0.8851\n",
            "2019-01-02 13:32:09 Epoch 10, Iter 0, loss=0.2025, acc=0.9333\n",
            "2019-01-02 13:33:45 Epoch 10, Iter 50, loss=0.1644, acc=0.9000\n",
            "2019-01-02 13:35:20 Epoch 10, Iter 100, loss=0.1872, acc=0.9667\n",
            "2019-01-02 13:36:55 Epoch 10, Iter 150, loss=0.4714, acc=0.7333\n",
            "2019-01-02 13:38:31 Epoch 10, Iter 200, loss=0.5573, acc=0.7000\n",
            "2019-01-02 13:40:06 Epoch 10, Iter 250, loss=0.4617, acc=0.9000\n",
            "2019-01-02 13:41:41 Epoch 10, Iter 300, loss=0.2332, acc=0.8333\n",
            "2019-01-02 13:43:16 Epoch 10, Iter 350, loss=0.3799, acc=0.8000\n",
            "2019-01-02 13:44:52 Epoch 10, Iter 400, loss=0.3296, acc=0.7667\n",
            "2019-01-02 13:46:27 Epoch 10, Iter 450, loss=0.3670, acc=0.9000\n",
            "2019-01-02 13:48:02 Epoch 10, Iter 500, loss=0.2664, acc=0.7333\n",
            "2019-01-02 13:49:38 Epoch 10, Iter 550, loss=0.3847, acc=0.7333\n",
            "2019-01-02 13:51:13 Epoch 10, Iter 600, loss=0.2753, acc=0.8667\n",
            "2019-01-02 13:52:48 Epoch 10, Iter 650, loss=0.1967, acc=0.8333\n",
            "2019-01-02 13:54:23 Epoch 10, Iter 700, loss=0.3139, acc=0.8333\n",
            "2019-01-02 13:55:59 Epoch 10, Iter 750, loss=0.3527, acc=0.7667\n",
            "2019-01-02 13:57:34 Epoch 10, Iter 800, loss=0.1884, acc=0.9333\n",
            "2019-01-02 13:59:10 Epoch 10, Iter 850, loss=0.3309, acc=0.7667\n",
            "2019-01-02 14:00:45 Epoch 10, Iter 900, loss=0.3263, acc=0.7667\n",
            "2019-01-02 14:01:28 Epoch 10, train_loss=0.2902, train_accuracy=0.8496\n",
            "2019-01-02 14:04:03 Epoch 10, valid_loss=0.3653, valid_accuracy=0.8806\n",
            "2019-01-02 14:04:05 Epoch 11, Iter 0, loss=0.2155, acc=0.9667\n",
            "2019-01-02 14:05:40 Epoch 11, Iter 50, loss=0.2942, acc=0.7333\n",
            "2019-01-02 14:07:15 Epoch 11, Iter 100, loss=0.2245, acc=0.8667\n",
            "2019-01-02 14:08:51 Epoch 11, Iter 150, loss=0.2097, acc=0.8667\n",
            "2019-01-02 14:10:26 Epoch 11, Iter 200, loss=0.2211, acc=0.8000\n",
            "2019-01-02 14:12:01 Epoch 11, Iter 250, loss=0.2775, acc=0.8667\n",
            "2019-01-02 14:13:36 Epoch 11, Iter 300, loss=0.3121, acc=0.8667\n",
            "2019-01-02 14:15:12 Epoch 11, Iter 350, loss=0.2340, acc=0.8333\n",
            "2019-01-02 14:16:47 Epoch 11, Iter 400, loss=0.3835, acc=0.8667\n",
            "2019-01-02 14:18:23 Epoch 11, Iter 450, loss=0.3426, acc=0.8333\n",
            "2019-01-02 14:19:58 Epoch 11, Iter 500, loss=0.3388, acc=0.8000\n",
            "2019-01-02 14:21:33 Epoch 11, Iter 550, loss=0.2224, acc=0.9333\n",
            "2019-01-02 14:23:08 Epoch 11, Iter 600, loss=0.1902, acc=0.8667\n",
            "2019-01-02 14:24:44 Epoch 11, Iter 650, loss=0.2687, acc=0.8000\n",
            "2019-01-02 14:26:19 Epoch 11, Iter 700, loss=0.6114, acc=0.7000\n",
            "2019-01-02 14:27:54 Epoch 11, Iter 750, loss=0.6672, acc=0.8333\n",
            "2019-01-02 14:29:29 Epoch 11, Iter 800, loss=0.3986, acc=0.8333\n",
            "2019-01-02 14:31:05 Epoch 11, Iter 850, loss=0.3268, acc=0.8333\n",
            "2019-01-02 14:32:40 Epoch 11, Iter 900, loss=0.2708, acc=0.9000\n",
            "2019-01-02 14:33:24 Epoch 11, train_loss=0.3220, train_accuracy=0.8484\n",
            "2019-01-02 14:35:58 Epoch 11, valid_loss=0.3783, valid_accuracy=0.8710\n",
            "2019-01-02 14:36:00 Epoch 12, Iter 0, loss=0.5466, acc=0.7333\n",
            "2019-01-02 14:37:35 Epoch 12, Iter 50, loss=0.2462, acc=0.8333\n",
            "2019-01-02 14:39:11 Epoch 12, Iter 100, loss=0.3900, acc=0.8333\n",
            "2019-01-02 14:40:46 Epoch 12, Iter 150, loss=0.5285, acc=0.8667\n",
            "2019-01-02 14:42:21 Epoch 12, Iter 200, loss=0.3018, acc=0.8333\n",
            "2019-01-02 14:43:57 Epoch 12, Iter 250, loss=0.2134, acc=0.8333\n",
            "2019-01-02 14:45:32 Epoch 12, Iter 300, loss=0.3459, acc=0.8667\n",
            "2019-01-02 14:47:07 Epoch 12, Iter 350, loss=0.3480, acc=0.7667\n",
            "2019-01-02 14:48:43 Epoch 12, Iter 400, loss=0.2794, acc=0.8000\n",
            "2019-01-02 14:50:18 Epoch 12, Iter 450, loss=0.4274, acc=0.8000\n",
            "2019-01-02 14:51:53 Epoch 12, Iter 500, loss=0.4866, acc=0.8000\n",
            "2019-01-02 14:53:29 Epoch 12, Iter 550, loss=0.5528, acc=0.8000\n",
            "2019-01-02 14:55:04 Epoch 12, Iter 600, loss=0.3014, acc=0.8667\n",
            "2019-01-02 14:56:39 Epoch 12, Iter 650, loss=0.2486, acc=0.8667\n",
            "2019-01-02 14:58:15 Epoch 12, Iter 700, loss=0.3667, acc=0.8000\n",
            "2019-01-02 14:59:50 Epoch 12, Iter 750, loss=0.4214, acc=0.8667\n",
            "2019-01-02 15:01:25 Epoch 12, Iter 800, loss=0.4919, acc=0.7333\n",
            "2019-01-02 15:03:01 Epoch 12, Iter 850, loss=0.2494, acc=0.8667\n",
            "2019-01-02 15:04:36 Epoch 12, Iter 900, loss=0.5035, acc=0.7333\n",
            "2019-01-02 15:05:20 Epoch 12, train_loss=0.3628, train_accuracy=0.8253\n",
            "2019-01-02 15:07:53 Epoch 12, valid_loss=0.4147, valid_accuracy=0.8541\n",
            "2019-01-02 15:07:56 Epoch 13, Iter 0, loss=0.4898, acc=0.7667\n",
            "2019-01-02 15:09:31 Epoch 13, Iter 50, loss=0.2955, acc=0.7667\n",
            "2019-01-02 15:11:06 Epoch 13, Iter 100, loss=0.4828, acc=0.7333\n",
            "2019-01-02 15:12:42 Epoch 13, Iter 150, loss=0.3940, acc=0.8000\n",
            "2019-01-02 15:14:17 Epoch 13, Iter 200, loss=0.6065, acc=0.7333\n",
            "2019-01-02 15:15:52 Epoch 13, Iter 250, loss=0.4067, acc=0.8667\n",
            "2019-01-02 15:17:27 Epoch 13, Iter 300, loss=0.4374, acc=0.8333\n",
            "2019-01-02 15:19:03 Epoch 13, Iter 350, loss=0.3823, acc=0.8333\n",
            "2019-01-02 15:20:38 Epoch 13, Iter 400, loss=0.3051, acc=0.9000\n",
            "2019-01-02 15:22:13 Epoch 13, Iter 450, loss=0.2641, acc=0.8667\n",
            "2019-01-02 15:23:48 Epoch 13, Iter 500, loss=0.4992, acc=0.8333\n",
            "2019-01-02 15:25:24 Epoch 13, Iter 550, loss=0.4756, acc=0.7333\n",
            "2019-01-02 15:26:59 Epoch 13, Iter 600, loss=0.8009, acc=0.8000\n",
            "2019-01-02 15:28:34 Epoch 13, Iter 650, loss=0.2618, acc=0.8667\n",
            "2019-01-02 15:30:10 Epoch 13, Iter 700, loss=0.4347, acc=0.8000\n",
            "2019-01-02 15:31:45 Epoch 13, Iter 750, loss=0.4274, acc=0.8333\n",
            "2019-01-02 15:33:20 Epoch 13, Iter 800, loss=0.4086, acc=0.8000\n",
            "2019-01-02 15:34:56 Epoch 13, Iter 850, loss=0.4601, acc=0.8333\n",
            "2019-01-02 15:36:31 Epoch 13, Iter 900, loss=0.2746, acc=0.9000\n",
            "2019-01-02 15:37:15 Epoch 13, train_loss=0.3885, train_accuracy=0.8230\n",
            "2019-01-02 15:39:49 Epoch 13, valid_loss=0.3959, valid_accuracy=0.8673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PyH2SbvbIWxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        " ### 自行添加：/bst_model、/log内文件保存到google drive或本地\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1GAoZvBEI_RN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TO DO\n",
        "# !cp bst_model/en_5e-05_2018-12-23_04-48-53.pth gdrive/My\\ Drive/en_5e-05_2018-12-23_04-48-53.pth\n",
        "# cp bst_model/cn_0.001_2019-01-02_08-12-47.pth gdrive/My\\ Drive/cn_attention.pth"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}